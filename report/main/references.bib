
@article{fujimoto_addressing_2018,
  title    = {Addressing {Function} {Approximation} {Error} in {Actor}-{Critic} {Methods}},
  url      = {http://arxiv.org/abs/1802.09477},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  urldate  = {2021-07-15},
  journal  = {arXiv:1802.09477 [cs, stat]},
  author   = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  month    = oct,
  year     = {2018},
  note     = {arXiv: 1802.09477},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}


@article{lillicrap_continuous_2019,
  title    = {Continuous control with deep reinforcement learning},
  url      = {http://arxiv.org/abs/1509.02971},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  urldate  = {2021-06-17},
  journal  = {arXiv:1509.02971 [cs, stat]},
  author   = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  month    = jul,
  year     = {2019},
  note     = {arXiv: 1509.02971},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}


@misc{zhao_product_2017,
  title    = {Product {Demand}},
  url      = {https://kaggle.com/felixzhao/productdemandforecasting/version/1},
  abstract = {Historical product demand for a manufacturing company with footprints globally},
  language = {en},
  urldate  = {2021-07-15},
  author   = {Zhao, Felix},
  month    = aug,
  year     = {2017},
}